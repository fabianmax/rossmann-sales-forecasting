{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "from pytorch_forecasting import TemporalFusionTransformer, TimeSeriesDataSet\n",
    "from pytorch_forecasting.data import GroupNormalizer\n",
    "from pytorch_forecasting.metrics import SMAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = 'data/prepared'\n",
    "SUBMISSION_PATH = 'data/submission'\n",
    "\n",
    "USE_LOG = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pickled data\n",
    "train_df = pd.read_pickle(f'{DATA_PATH}/train_df.pkl')\n",
    "test_df = pd.read_pickle(f'{DATA_PATH}/test_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch Forecasting needs a continuing index to indicate time steps\n",
    "# Create it as the difference from the first day in the dataset in days\n",
    "train_df = train_df.sort_values(['Store', 'Date'], ascending = True)\n",
    "test_df = test_df.sort_values(['Store', 'Date'], ascending = True)\n",
    "\n",
    "first_day = train_df['Date'].min()\n",
    "\n",
    "train_df['time_idx'] = (train_df['Date'] - first_day).dt.days\n",
    "test_df['time_idx'] = (test_df['Date'] - first_day).dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TFT can use embeddings for categorical features, therefore no need for\n",
    "# the one-hot-encoding (need to reverse it)\n",
    "\n",
    "def reverse_onehot(data, col):\n",
    "    \"\"\"\n",
    "    Reverse one-hot encoding\n",
    "    \"\"\"\n",
    "    # Extract a list of column names that match the regex\n",
    "    cols = [x for x in data.columns if x.startswith(col)]\n",
    "\n",
    "    data[col] = ''\n",
    "\n",
    "    for col_name in cols:\n",
    "        data.loc[data[col_name] == 1, col] = col_name\n",
    "\n",
    "    data = data.drop(cols, axis=1)\n",
    "\n",
    "    return data\n",
    "\n",
    "train_df = reverse_onehot(train_df, 'StateHoliday')\n",
    "train_df = reverse_onehot(train_df, 'StoreType')\n",
    "train_df = reverse_onehot(train_df, 'Assortment')\n",
    "\n",
    "test_df = reverse_onehot(test_df, 'StateHoliday')\n",
    "test_df = reverse_onehot(test_df, 'StoreType')\n",
    "test_df = reverse_onehot(test_df, 'Assortment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TFT needs special category dtypes, transform for relevant columns\n",
    "\n",
    "def to_category(data, cols):\n",
    "    \"\"\"\n",
    "    Convert columns to category\n",
    "    \"\"\"\n",
    "    for col in cols:\n",
    "        data[col] = data[col].astype(str).astype('category')\n",
    "\n",
    "    return data\n",
    "\n",
    "cat_cols_ls = ['Year', 'Month', 'DayOfWeek', 'WeekOfYear', 'Store']\n",
    "\n",
    "train_df = to_category(train_df, cat_cols_ls)\n",
    "test_df = to_category(test_df, cat_cols_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on log scale to handle skewed target distribution\n",
    "if USE_LOG:\n",
    "    train_df['Sales'] = np.log1p(train_df['Sales'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Max prediction length is the length of the prediction task in test\n",
    "max_prediction_length = int(test_df.groupby('Store').Store.count().unique()[0])\n",
    "\n",
    "# Max encoder length defined as multiple of the prediction length\n",
    "max_encoder_length = max_prediction_length * 4\n",
    "\n",
    "training = TimeSeriesDataSet(\n",
    "    train_df,\n",
    "    time_idx='time_idx',\n",
    "    target='Sales',\n",
    "    group_ids=['Store'],\n",
    "    min_encoder_length=max_encoder_length // 2,\n",
    "    max_encoder_length=max_encoder_length,\n",
    "    min_prediction_length=1,\n",
    "    max_prediction_length=max_prediction_length,\n",
    "    static_categoricals=[\n",
    "        'Store', \n",
    "        'Assortment', \n",
    "        'StoreType' \n",
    "        ],\n",
    "    static_reals=[\n",
    "        'CompetitionDistance',\n",
    "        'CompetitionOpenSinceMonth',\n",
    "        'CompetitionOpenSinceYear',\n",
    "    ],\n",
    "    time_varying_known_categoricals=[\n",
    "        'Year', \n",
    "        'Month', \n",
    "        'DayOfWeek', \n",
    "        'WeekOfYear'],\n",
    "    time_varying_known_reals=[\n",
    "        'time_idx',\n",
    "        'Promo2', \n",
    "        'Promo2SinceWeek',\n",
    "        'Promo2SinceYear',\n",
    "        'CompetitionOpen', \n",
    "        'PromoOpen', \n",
    "        'IsPromoMonth'\n",
    "        ],\n",
    "    time_varying_unknown_categoricals=[],\n",
    "    time_varying_unknown_reals=[\n",
    "        'Sales', \n",
    "        'Customers'\n",
    "        ],\n",
    "    target_normalizer=GroupNormalizer(\n",
    "        groups=['Store'], transformation='softplus'\n",
    "    ),\n",
    "    add_relative_time_idx=True,\n",
    "    add_target_scales=True,\n",
    "    add_encoder_length=True,\n",
    "    allow_missing_timesteps=True\n",
    ")\n",
    "\n",
    "# Create validation set (predict=True) which means to predict the \n",
    "# last max_prediction_length points in time for each series\n",
    "validation = TimeSeriesDataSet.from_dataset(training, \n",
    "                                            train_df, \n",
    "                                            predict=True, \n",
    "                                            stop_randomization=True)\n",
    "\n",
    "# Create dataloaders for model\n",
    "batch_size = 64\n",
    "train_dataloader = training.to_dataloader(train=True, \n",
    "                                          batch_size=batch_size, \n",
    "                                          num_workers=0)\n",
    "val_dataloader = validation.to_dataloader(train=False, \n",
    "                                          batch_size=batch_size * 10, \n",
    "                                          num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "/Users/fabianmueller/opt/anaconda3/envs/rossmann/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:735: LightningDeprecationWarning: `trainer.fit(train_dataloader)` is deprecated in v1.4 and will be removed in v1.6. Use `trainer.fit(train_dataloaders)` instead. HINT: added 's'\n",
      "  rank_zero_deprecation(\n",
      "\n",
      "   | Name                               | Type                            | Params\n",
      "----------------------------------------------------------------------------------------\n",
      "0  | loss                               | SMAPE                           | 0     \n",
      "1  | logging_metrics                    | ModuleList                      | 0     \n",
      "2  | input_embeddings                   | MultiEmbedding                  | 18.8 K\n",
      "3  | prescalers                         | ModuleDict                      | 256   \n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 4.3 K \n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 7.9 K \n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 6.3 K \n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 1.1 K \n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 1.1 K \n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 1.1 K \n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 1.1 K \n",
      "11 | lstm_encoder                       | LSTM                            | 2.2 K \n",
      "12 | lstm_decoder                       | LSTM                            | 2.2 K \n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 544   \n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 32    \n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 1.4 K \n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 1.1 K \n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 576   \n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 1.1 K \n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 576   \n",
      "20 | output_layer                       | Linear                          | 17    \n",
      "----------------------------------------------------------------------------------------\n",
      "51.2 K    Trainable params\n",
      "0         Non-trainable params\n",
      "51.2 K    Total params\n",
      "0.205     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fabianmueller/opt/anaconda3/envs/rossmann/lib/python3.9/site-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/Users/fabianmueller/opt/anaconda3/envs/rossmann/lib/python3.9/site-packages/pytorch_forecasting/data/timeseries.py:1657: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /Users/distiller/project/pytorch/torch/csrc/utils/tensor_new.cpp:210.)\n",
      "  target_scale = torch.tensor([batch[0][\"target_scale\"] for batch in batches], dtype=torch.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation sanity check:  50%|█████     | 1/2 [00:02<00:02,  2.40s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fabianmueller/opt/anaconda3/envs/rossmann/lib/python3.9/site-packages/pytorch_lightning/utilities/data.py:59: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 640. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "  warning_cache.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation sanity check: 100%|██████████| 2/2 [00:03<00:00,  1.41s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fabianmueller/opt/anaconda3/envs/rossmann/lib/python3.9/site-packages/pytorch_lightning/utilities/data.py:59: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 475. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "  warning_cache.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fabianmueller/opt/anaconda3/envs/rossmann/lib/python3.9/site-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/Users/fabianmueller/opt/anaconda3/envs/rossmann/lib/python3.9/site-packages/pytorch_lightning/trainer/data_loading.py:432: UserWarning: The number of training samples (30) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/32 [00:00<?, ?it/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fabianmueller/opt/anaconda3/envs/rossmann/lib/python3.9/site-packages/pytorch_lightning/utilities/data.py:59: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 64. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "  warning_cache.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: 100%|██████████| 32/32 [00:14<00:00,  2.17it/s, loss=0.346, v_num=6, train_loss_step=0.359, val_loss=0.299, train_loss_epoch=0.348]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Training callbacks\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    min_delta=1e-4, \n",
    "    patience=10, \n",
    "    verbose=False, \n",
    "    mode=\"min\")\n",
    "lr_logger = LearningRateMonitor()  \n",
    "logger = TensorBoardLogger(\"lightning_logs\")\n",
    "\n",
    "# PyTorch Ligntning trainer\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=30,\n",
    "    gpus=0,\n",
    "    weights_summary=\"top\",\n",
    "    gradient_clip_val=0.1,\n",
    "    limit_train_batches=30,\n",
    "    callbacks=[lr_logger, early_stop_callback],\n",
    "    logger=logger,\n",
    "    default_root_dir='model_checkpoints'\n",
    ")\n",
    "\n",
    "# TFT Model\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    training,\n",
    "    learning_rate=0.03,\n",
    "    hidden_size=16,\n",
    "    attention_head_size=1,\n",
    "    dropout=0.1,\n",
    "    hidden_continuous_size=8,\n",
    "    output_size=1,\n",
    "    loss=SMAPE(),\n",
    "    log_interval=10,\n",
    "    reduce_on_plateau_patience=4,\n",
    ")\n",
    "\n",
    "# Run training\n",
    "trainer.fit(\n",
    "    tft,\n",
    "    train_dataloader=train_dataloader,\n",
    "    val_dataloaders=val_dataloader,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stores to predict\n",
    "stores_in_test = test_df['Store'].unique()\n",
    "\n",
    "# Select the encoder data from the actual training data\n",
    "encoder_df = train_df[lambda x: x.time_idx > x.time_idx.max() - max_encoder_length]\n",
    "encoder_df = encoder_df[encoder_df['Store'].isin(stores_in_test)]\n",
    "\n",
    "# Define decoder data (taken from test data)\n",
    "decoder_df = test_df\n",
    "\n",
    "# Combine encoder and decoder data\n",
    "new_prediction_data = pd.concat([encoder_df, decoder_df], ignore_index=True)\n",
    "\n",
    "# Set future unknown to arbitrary value\n",
    "new_prediction_data[['Sales', 'Customers']] = new_prediction_data[['Sales', 'Customers']].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on test data\n",
    "y_hat, y_hat_idx = tft.predict(new_prediction_data, mode='prediction', return_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create submission file\n",
    "submission_df = pd.concat([y_hat_idx, pd.DataFrame(y_hat)], axis=1)\n",
    "submission_df = pd.melt(submission_df, id_vars=['time_idx', 'Store'], var_name='step', value_name='Sales')\n",
    "\n",
    "submission_df['time_idx'] = submission_df['time_idx'] + submission_df['step']\n",
    "submission_df = submission_df.sort_values(by=['Store', 'time_idx'])\n",
    "\n",
    "submission_df = pd.merge(submission_df, test_df[['Id', 'Store', 'time_idx']], on=['time_idx', 'Store'], how='left')\n",
    "\n",
    "submission_df['Sales'] = np.expm1(submission_df['Sales']).clip(0, None)\n",
    "submission_df = submission_df[['Id', 'Sales']]\n",
    "\n",
    "# Quick check\n",
    "assert submission_df.shape[0] == test_df.shape[0]\n",
    "\n",
    "submission_df.to_csv(f'{SUBMISSION_PATH}/submission_tft.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "82d2047610b1daa4448cbca375db7b6bf136c8e63867fbd16061ccfd9df8834a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('rossmann')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
